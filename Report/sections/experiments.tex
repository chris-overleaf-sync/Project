\section{Experiments}
\subsection{Experimental setup}
%Describe the setup (madry defense, our model attacks)
%Describe the dataset and test set
In order to test our attacks, we attack the Madry et al. MNIST challenge models \cite{madry2017towards}. The Madry Lab provides 3 defense models, a method for running a PGD attack, and standardized methods to retrieve the MNIST test data set and run attacks against the models. The MNIST dataset is a set of handwritten digits from 0-9, pre-split into 60k training samples and 10k testing samples. The challenge is to alter all test images up to $\epsilon=0.3$ distance, determined using the $l_\infty$ norm, with the goal of reducing the model's accuracy.

The Madry Models are composed of two convolutional layers, a relu layer, and a softmax output layer. There are three versions, one trained on natural samples, and two trained on natural samples and adversarial samples. The two trained on adversarial samples have different weights due to random initializations, though otherwise are the same. 

In our evaluation we attack the public adversarially trained model provided in the Madry Challenge. We attack it with a white-box methodology, meaning we use the knowledge of the model's layers and weights to generate the adversarial examples.

For a baseline, we run the four attacks described in the existing work, FGSM, I-FGSM, PGD, and MI-FGSM. We use the cleverhans library \cite{GoodfellowPM16}, a library made by Goodfellow and others that provides these standard adversarial attacks. For using this library, we pass in the model layers and weights and the 10k training samples and extract the modified adversarial examples. We then use the Madry Lab evaluation to determine the accuracy of the model against these generated examples. We run a hyperparameter search for each model around the default values or reported best values.

We follow the same procedure for our model with a more extensive parameter search. A list of the attack accuracies and the most successful parameters are listed in table \ref{tab:res_table}.

\begin{table}
    \begin{center}
        \begin{tabular}{l|c|c|c|l}
            \hline
            Attack & Accuracy & Iterations & Step Size & Other Params\\
            \hline
            Baselines \\
            \hline
            None & 98.40 &&& \\
            FGSM & 96.23 & 1 & 0.3 & \\\
            I-FGSM & 93.21 & 100 & 0.01 & \\
            PGD & 92.52 & 100 & 0.01 & random starts: 50, cross entropy loss \\
            MI-FGSM & 92.83 & 100 & 0.1 & decay factor: 0.04 \\
            \hline
            PI-FGSM \\
            \hline
            Top Gradients & 92.39 & 64 & 0.02 & \#top grads: 512\\
            Distributed Gradients & 93.23 & 128 & 256 & \\
            Clipped Pixels & 92.31 & 128 & 0.04 & \#top grads: 128
        \end{tabular}
    \end{center}
    \caption{\label{tab:res_table}Attack accuracy and attack parameters against the public adversarial model.}
\end{table}