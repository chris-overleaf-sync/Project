\section{Introduction}
\subsection{Problem description}
As Deep Neural Networks (DNN) are used in more applications in a wider range of domains, reliability and security are becoming critical concerns for machine learning practitioners. The field of generating and defending from adversarial attacks has developed in recent years to address this concern \cite{szegedy2013intriguing, goodfellow6572explaining}. Adversarial images can be used to fool neural networks into incorrectly classifying examples with differences undetectable to the human eye, and these examples can be fed the classifier digitally or even through printed pictures \cite{kurakin2016adversarial}. Finding examples of adversarial images are useful because they can be used to train DNNs that are more resilient to adversarial attacks \cite{goodfellow6572explaining, madry2017towards} and they can lead us to a better understanding of the inner workings of DNNs.

Adversarial examples are generated by altering pixels of an image using information of an image. To determine the amount an image has been altered the distance between images can be calculated by using $l_1, l_2,$ or $l_\infty$ measures between images. Examples that are generated by attacking one neural network can often be used as adversarial examples against other neural networks as well\cite{szegedy2013intriguing}. This suggests that even different neural networks cannot currently generalize well given natural data sets. However, training with adversarial examples does help in defending against adversarial examples, to an extent \cite{goodfellow6572explaining}.

In this project we formulate a set of attack types which we call Partial Iterative Fast Gradient Sign Method (PI-FGSM). PI-FGSM is a class of attacks which are modifications of I-FGSM; instead of applying the full gradient we only apply part of the gradient such as the most significant portions of the gradient. For attacks that are limited to a specific $l_\infty$ perturbation, our intuition is that we can make more significant changes within that allowable range of perturbation by doing more iterations of limited changes.
\subsection{Existing work}
Several types of adversarial attacks have been designed since the original discovery of adversarial examples by Szegedy et al. \cite{szegedy2013intriguing}. Here is a brief overview of a few attacks that we use as baselines to compare our attack to:
\begin{itemize}
    \item Fast Gradient Sign Method (FGSM) described by Goodfellow et al. \cite{goodfellow6572explaining} uses the gradient of the output to perturb all pixels away from the gradient. This can also be used as a targeted attack by  perturbing the pixel in the direction of the gradient of a particular label.
    \item Iterative FGSM (I-FGSM) described by Kurakin et al. \cite{kurakin2016adversarial} takes a step approach to FGSM, incrementally changing the pixels by small perturbations and recalculating the gradient after each step.
    \item Projected Gradient Descent (PGD) described by Madry et al. \cite{madry2017towards} is a projected version of iterative FGSM, starting with a random initial perturbation and then repeating FGSM over a test example until the desired perturbation is achieved.
    \item Momentum Iterative FGSM (MI-FGSM) described by Dong et al. \cite{dong2017boosting} uses a momentum term to help searching for adversarial examples. The momentum term helps by pushing elements out of relative maxima, and a decay term ensures that the search narrows to a maxima.
\end{itemize}
While several defense mechanisms have been proposed, recent work \cite{carlini2017towards} has shown that despite promising results, existing defenses only marginally improve robustness against advanced adversaries. As a result, the best approach at hardening against adversarial attacks continues to be training a model using both regular and adversarial examples \cite{madry2017towards}. Madry et al. claim that PGD is a universal first-order adversary, one that utilizes first order network information. Utilizing this information, they train a robust network using PGD to create adversarial examples and publicly released it to test its robustness.